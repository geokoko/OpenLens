{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data processing (FER 2013 Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was taken from this source: https://www.kaggle.com/code/ayushsaini04/face-expression-recognition-with-deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images have a resolution 48x48. The images appearing in the plot are random images of the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input path for the images\n",
    "base_path = \"data/archive/images/\"\n",
    "# class names\n",
    "classes = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of images for each facial expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of images in the training split: \n",
      "3205 surprise images\n",
      "436 disgust images\n",
      "7164 happy images\n",
      "4938 sad images\n",
      "4103 fear images\n",
      "4982 neutral images\n",
      "3993 angry images\n",
      "\n",
      "No. of images in the validation split: \n",
      "797 surprise images\n",
      "111 disgust images\n",
      "1825 happy images\n",
      "1139 sad images\n",
      "1018 fear images\n",
      "1216 neutral images\n",
      "960 angry images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"No. of images in the training split: \")\n",
    "for expression in os.listdir(base_path + \"train\"):\n",
    "    print(str(len(os.listdir(base_path + \"train/\" + expression))) + \" \" + expression + \" images\")\n",
    "\n",
    "print(\"\\nNo. of images in the validation split: \")\n",
    "for expression in os.listdir(base_path + \"validation\"):\n",
    "    print(str(len(os.listdir(base_path + \"validation/\" + expression))) + \" \" + expression + \" images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"disgust\" images appear to be much lesser than the other categories. This significant imbalance could potentially cause problems to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good solution to this issue is to augment the images of the \"disgust\" dataset, so that we create more samples of this class and thus improve the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a random image and display it\n",
    "\n",
    "import imageio\n",
    "import imgaug as ia\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "disgust_path = \"data/archive/images/train/disgust/\"\n",
    "disgust_path_val = \"data/archive/images/validation/disgust/\"\n",
    "image = imageio.imread(disgust_path + random.choice(os.listdir(disgust_path)))\n",
    "\n",
    "print(\"Original: \")\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Augment the Image\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "ia.seed(4)\n",
    "\n",
    "# Define different sequential augmenters\n",
    "seq1 = iaa.Sequential([\n",
    "    iaa.Affine(rotate=45),\n",
    "    iaa.AdditiveGaussianNoise(scale=0.01*255)\n",
    "])\n",
    "\n",
    "seq2 = iaa.Sequential([\n",
    "    iaa.Fliplr(1)  # Horizontal flip\n",
    "])\n",
    "\n",
    "seq3 = iaa.Sequential([\n",
    "    iaa.GaussianBlur(sigma=(0.01, 0.05)),\n",
    "    iaa.Dropout(p=(0.01, 0.05)),\n",
    "])\n",
    "\n",
    "seq4 = iaa.Sequential([\n",
    "    iaa.Crop(percent=(0, 0.2)),  # random crops\n",
    "    iaa.Sharpen(alpha=(0.01, 0.05))  # sharpen the image\n",
    "])\n",
    "\n",
    "# List of all sequentials\n",
    "sequentials = [seq1, seq2, seq3, seq4]\n",
    "\n",
    "\n",
    "for i, seq in enumerate(sequentials, start=1):\n",
    "    # Apply augmentation\n",
    "    image_aug = seq(image=image)\n",
    "    \n",
    "    # Display the augmented image\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image_aug)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Augmented Image {i}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do the same thing as in the above cell but for all the images in the \"disgust\" folder, in order to increase the number of samples we are going to use for our training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In case we want to delete some of the images we just created, we can run this script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "disgust_path = \"data/archive/images/train/disgust/\" # add the path here\n",
    "disgust_path_val = \"data/archive/images/validation/disgust/\"\n",
    "# List all files in the directory\n",
    "files_in_directory = os.listdir(disgust_path) \n",
    "files_in_directory_val = os.listdir(disgust_path_val)\n",
    "files_to_delete = []\n",
    "\n",
    "# Filter for files that end with '_aug.jpg'\n",
    "for file in files_in_directory:    \n",
    "    for i in range(0, 5):\n",
    "        if file.endswith(f'_aug{i}.jpg'):\n",
    "            files_to_delete.append(file)\n",
    "\n",
    "# Delete the files\n",
    "for file in files_to_delete:\n",
    "    os.remove(os.path.join(disgust_path, file))\n",
    "    print(f\"Deleted {file}\")\n",
    "\n",
    "files_to_delete = []\n",
    "\n",
    "for file in files_in_directory_val:\n",
    "    for i in range(0, 5):\n",
    "        if file.endswith(f'_aug{i}.jpg'):\n",
    "            files_to_delete.append(file)\n",
    "\n",
    "for file in files_to_delete:\n",
    "    os.remove(os.path.join(disgust_path_val, file))\n",
    "    print(f\"Deleted {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_once=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_once == False:\n",
    "    for pic in os.listdir(disgust_path):\n",
    "        image = imageio.imread(disgust_path + pic)\n",
    "        for i, seq in enumerate(sequentials, start=1):\n",
    "            image_aug = seq(image=image)\n",
    "            imageio.imwrite(os.path.join(disgust_path, pic[:-4] + f\"_aug{i}.jpg\"), image_aug)\n",
    "    \n",
    "    for pic in os.listdir(disgust_path_val):\n",
    "        image = imageio.imread(disgust_path_val + pic)\n",
    "        for i, seq in enumerate(sequentials, start=1):\n",
    "            image_aug = seq(image=image)\n",
    "            imageio.imwrite(os.path.join(disgust_path_val, pic[:-4] + f\"_aug{i}.jpg\"), image_aug)\n",
    "if run_once == True:\n",
    "    print(\"Augmentation already done\")\n",
    "run_once = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No. of images in the training split: \")\n",
    "for expression in os.listdir(base_path + \"train\"):\n",
    "    print(str(len(os.listdir(base_path + \"train/\" + expression))) + \" \" + expression + \" images\")\n",
    "\n",
    "print(\"\\nNo. of images in the validation split: \")\n",
    "for expression in os.listdir(base_path + \"validation\"):\n",
    "    print(str(len(os.listdir(base_path + \"validation/\" + expression))) + \" \" + expression + \" images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that our dataset is more balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if train_on_gpu else \"cpu\")\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we load the dataset with Data Loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision import transforms, datasets, utils\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(), # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize the pixel values\n",
    "])\n",
    "\n",
    "# input path for the images\n",
    "base_path = \"data/archive/images/\"\n",
    "\n",
    "# defining train dataset\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(base_path, \"train\"), transform=transform)\n",
    "# defining validation dataset\n",
    "val_dataset = datasets.ImageFolder(root=os.path.join(base_path, \"validation\"), transform=transform)\n",
    "\n",
    "# creating a DataLoader for the train dataset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# function to display images\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get a batch of images from train dataset\n",
    "train_dataiter = iter(train_dataloader)\n",
    "images, labels = next(train_dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(utils.make_grid(images))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        # Load a pre-trained ResNet-18 model. Inittially, the model is trained on ImageNet dataset\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Remove the fully connected layers of ResNet-18 and use it purely as a feature extractor\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])\n",
    "        \n",
    "        # Custom layers\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(512, 256)  # ResNet-18 features to 256\n",
    "        self.fc2 = nn.Linear(256, num_classes)  # Final output layer\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = self.resnet(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Custom classifier\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.25, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we define an optimizer and a loss function, and then we ensure each batch of input data and labels are sent to GPU during our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model = EmotionCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss() # Cross-entropy loss for classification problems\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device) # transfer image data and labels to GPU\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_dataloader)}')\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Set predictions and labels lists\n",
    "    all_preds = [] \n",
    "    all_labels = []\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device) # tranfer image data and labels to GPU\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Collect for F1 calculation\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on the validation set: {accuracy}%')\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')  # 'weighted' accounts for label imbalance\n",
    "    print(f'F1 Score on the validation set: {f1}')\n",
    "\n",
    "    # Save the model if it has the best accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Saved new best model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Real-Time Webcam Feed Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'best_model.pth'\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "_model = EmotionCNN().to(device)\n",
    "_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "_model.eval() # set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotion labels\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "# Start the webcam\n",
    "cap2 = cv2.VideoCapture(0) # the 0 is the camera index, might need change depending on the setup\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(), # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize the pixel values\n",
    "])\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap2.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    #print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
    "    if torch.cuda.is_available():\n",
    "        frame_processed = transform(frame_rgb)\n",
    "        frame_processed = frame_processed.unsqueeze(0).to(device) # send preprocessed frame to GPU\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = _model(frame_processed)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)[0]  # Apply softmax to convert to probabilities\n",
    "        probabilities = probabilities.cpu().numpy()  # Convert to numpy array\n",
    "\n",
    "    # Get the label of the highest probability\n",
    "    predicted_emotion = emotion_labels[probabilities.argmax()]\n",
    "\n",
    "    for i, (emotion, score) in enumerate(zip(emotion_labels, probabilities)):\n",
    "        label = \"{}: {:.2f}%\".format(emotion, score * 100)\n",
    "        cv2.putText(frame, label, (10, 20 + i * 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (100, 255, 100), 1)\n",
    "\n",
    "    cv2.imshow('Emotion Detection', frame_rgb)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap2.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
